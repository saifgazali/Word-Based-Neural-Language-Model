{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Based Neural Language model\n",
    "\n",
    "The nursery rhyme Sing a Song of Sixpence is well known in the west. The first verse is common,\n",
    "but there is also a 4 verse version that we will use to develop our character-based language\n",
    "model. It is short, so fitting the model will be fast, but not so short that we wonâ€™t see anything\n",
    "interesting. The complete 4 verse version we will use as source text is listed below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model Design "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must train our language model on a text and in the case of a character based language model, the input and output sequence must be characters\n",
    "\n",
    "#The number of characters used as input will also define the number of characters that will need to be provided to the model\n",
    "#in order to elicit the first predicted character. After the first character has been generated, it\n",
    "#can be appended to the input sequence and used as input for the model to generate the next character.\n",
    "\n",
    "\n",
    "\n",
    "# Longer sequences offer more context but take longer to train. \n",
    "# We will use an arbitary length of 10 characters for this model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sing a song of sixpence,\n",
      "A pocket full of rye.\n",
      "Four and twenty blackbirds,\n",
      "Baked in a pie.\n",
      "When the pie was opened\n",
      "The birds began to sing;\n",
      "Wasn't that a dainty dish,\n",
      "To set before the king.\n",
      "The king was in his counting house,\n",
      "Counting out his money;\n",
      "The queen was in the parlour,\n",
      "Eating bread and honey.\n",
      "The maid was in the garden,\n",
      "Hanging out the clothes,\n",
      "When down came a blackbird\n",
      "And pecked off her nose.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_text = load_doc('ryhme.txt')\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data\n",
    "\n",
    "we will strip all of the new line characters so we will have one long sequence of characters separater only by white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def clean_data(text):\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in string.punctuation]\n",
    "    tokens = [t for t in tokens if t.isalpha()]\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = clean_data(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sing a song of a pocket full of four and twenty baked in a when the pie was opened the birds began to that a dainty to set before the the king was in his counting counting out his the queen was in the eating bread and the maid was in the hanging out the when down came a blackbird and pecked off her\n"
     ]
    }
   ],
   "source": [
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each input sequence will be 10 characters long with one output character, making each seq 11 characters long\n",
    "length = 10\n",
    "sequences = list()\n",
    "def create_seq(raw_text):\n",
    "    for i in range(length,len(raw_text)):\n",
    "        sequences.append(raw_text[i-length:i+1])\n",
    "    print('Total Sequences',len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences 289\n"
     ]
    }
   ],
   "source": [
    "create_seq(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sing a song', 'ing a song ', 'ng a song o', 'g a song of', ' a song of ', 'a song of a', ' song of a ', 'song of a p', 'ong of a po', 'ng of a poc', 'g of a pock', ' of a pocke', 'of a pocket', 'f a pocket ', ' a pocket f', 'a pocket fu', ' pocket ful', 'pocket full', 'ocket full ', 'cket full o', 'ket full of', 'et full of ', 't full of f', ' full of fo', 'full of fou', 'ull of four', 'll of four ', 'l of four a', ' of four an', 'of four and', 'f four and ', ' four and t', 'four and tw', 'our and twe', 'ur and twen', 'r and twent', ' and twenty', 'and twenty ', 'nd twenty b', 'd twenty ba', ' twenty bak', 'twenty bake', 'wenty baked', 'enty baked ', 'nty baked i', 'ty baked in', 'y baked in ', ' baked in a', 'baked in a ', 'aked in a w', 'ked in a wh', 'ed in a whe', 'd in a when', ' in a when ', 'in a when t', 'n a when th', ' a when the', 'a when the ', ' when the p', 'when the pi', 'hen the pie', 'en the pie ', 'n the pie w', ' the pie wa', 'the pie was', 'he pie was ', 'e pie was o', ' pie was op', 'pie was ope', 'ie was open', 'e was opene', ' was opened', 'was opened ', 'as opened t', 's opened th', ' opened the', 'opened the ', 'pened the b', 'ened the bi', 'ned the bir', 'ed the bird', 'd the birds', ' the birds ', 'the birds b', 'he birds be', 'e birds beg', ' birds bega', 'birds began', 'irds began ', 'rds began t', 'ds began to', 's began to ', ' began to t', 'began to th', 'egan to tha', 'gan to that', 'an to that ', 'n to that a', ' to that a ', 'to that a d', 'o that a da', ' that a dai', 'that a dain', 'hat a daint', 'at a dainty', 't a dainty ', ' a dainty t', 'a dainty to', ' dainty to ', 'dainty to s', 'ainty to se', 'inty to set', 'nty to set ', 'ty to set b', 'y to set be', ' to set bef', 'to set befo', 'o set befor', ' set before', 'set before ', 'et before t', 't before th', ' before the', 'before the ', 'efore the t', 'fore the th', 'ore the the', 're the the ', 'e the the k', ' the the ki', 'the the kin', 'he the king', 'e the king ', ' the king w', 'the king wa', 'he king was', 'e king was ', ' king was i', 'king was in', 'ing was in ', 'ng was in h', 'g was in hi', ' was in his', 'was in his ', 'as in his c', 's in his co', ' in his cou', 'in his coun', 'n his count', ' his counti', 'his countin', 'is counting', 's counting ', ' counting c', 'counting co', 'ounting cou', 'unting coun', 'nting count', 'ting counti', 'ing countin', 'ng counting', 'g counting ', ' counting o', 'counting ou', 'ounting out', 'unting out ', 'nting out h', 'ting out hi', 'ing out his', 'ng out his ', 'g out his t', ' out his th', 'out his the', 'ut his the ', 't his the q', ' his the qu', 'his the que', 'is the quee', 's the queen', ' the queen ', 'the queen w', 'he queen wa', 'e queen was', ' queen was ', 'queen was i', 'ueen was in', 'een was in ', 'en was in t', 'n was in th', ' was in the', 'was in the ', 'as in the e', 's in the ea', ' in the eat', 'in the eati', 'n the eatin', ' the eating', 'the eating ', 'he eating b', 'e eating br', ' eating bre', 'eating brea', 'ating bread', 'ting bread ', 'ing bread a', 'ng bread an', 'g bread and', ' bread and ', 'bread and t', 'read and th', 'ead and the', 'ad and the ', 'd and the m', ' and the ma', 'and the mai', 'nd the maid', 'd the maid ', ' the maid w', 'the maid wa', 'he maid was', 'e maid was ', ' maid was i', 'maid was in', 'aid was in ', 'id was in t', 'd was in th', ' was in the', 'was in the ', 'as in the h', 's in the ha', ' in the han', 'in the hang', 'n the hangi', ' the hangin', 'the hanging', 'he hanging ', 'e hanging o', ' hanging ou', 'hanging out', 'anging out ', 'nging out t', 'ging out th', 'ing out the', 'ng out the ', 'g out the w', ' out the wh', 'out the whe', 'ut the when', 't the when ', ' the when d', 'the when do', 'he when dow', 'e when down', ' when down ', 'when down c', 'hen down ca', 'en down cam', 'n down came', ' down came ', 'down came a', 'own came a ', 'wn came a b', 'n came a bl', ' came a bla', 'came a blac', 'ame a black', 'me a blackb', 'e a blackbi', ' a blackbir', 'a blackbird', ' blackbird ', 'blackbird a', 'lackbird an', 'ackbird and', 'ckbird and ', 'kbird and p', 'bird and pe', 'ird and pec', 'rd and peck', 'd and pecke', ' and pecked', 'and pecked ', 'nd pecked o', 'd pecked of', ' pecked off', 'pecked off ', 'ecked off h', 'cked off he', 'ked off her']\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_doc(lines,filename):\n",
    "    text = '\\n'.join(lines)\n",
    "    file = open(filename,'w')\n",
    "    file.write(text)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'char_seq.txt'\n",
    "save_doc(sequences,output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will develop a neural language model for the prepared sequence data. The\n",
    "model will read encoded characters and predict the next character in the sequence. A Long\n",
    "Short-Term Memory recurrent neural network hidden layer will be used to learn the context\n",
    "from the input sequence in order to make the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_filename = 'char_seq.txt'\n",
    "raw_text = load_doc(in_filename)\n",
    "lines = raw_text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sing a song', 'ing a song ', 'ng a song o', 'g a song of', ' a song of ', 'a song of a', ' song of a ', 'song of a p', 'ong of a po', 'ng of a poc', 'g of a pock', ' of a pocke', 'of a pocket', 'f a pocket ', ' a pocket f', 'a pocket fu', ' pocket ful', 'pocket full', 'ocket full ', 'cket full o', 'ket full of', 'et full of ', 't full of f', ' full of fo', 'full of fou', 'ull of four', 'll of four ', 'l of four a', ' of four an', 'of four and', 'f four and ', ' four and t', 'four and tw', 'our and twe', 'ur and twen', 'r and twent', ' and twenty', 'and twenty ', 'nd twenty b', 'd twenty ba', ' twenty bak', 'twenty bake', 'wenty baked', 'enty baked ', 'nty baked i', 'ty baked in', 'y baked in ', ' baked in a', 'baked in a ', 'aked in a w', 'ked in a wh', 'ed in a whe', 'd in a when', ' in a when ', 'in a when t', 'n a when th', ' a when the', 'a when the ', ' when the p', 'when the pi', 'hen the pie', 'en the pie ', 'n the pie w', ' the pie wa', 'the pie was', 'he pie was ', 'e pie was o', ' pie was op', 'pie was ope', 'ie was open', 'e was opene', ' was opened', 'was opened ', 'as opened t', 's opened th', ' opened the', 'opened the ', 'pened the b', 'ened the bi', 'ned the bir', 'ed the bird', 'd the birds', ' the birds ', 'the birds b', 'he birds be', 'e birds beg', ' birds bega', 'birds began', 'irds began ', 'rds began t', 'ds began to', 's began to ', ' began to t', 'began to th', 'egan to tha', 'gan to that', 'an to that ', 'n to that a', ' to that a ', 'to that a d', 'o that a da', ' that a dai', 'that a dain', 'hat a daint', 'at a dainty', 't a dainty ', ' a dainty t', 'a dainty to', ' dainty to ', 'dainty to s', 'ainty to se', 'inty to set', 'nty to set ', 'ty to set b', 'y to set be', ' to set bef', 'to set befo', 'o set befor', ' set before', 'set before ', 'et before t', 't before th', ' before the', 'before the ', 'efore the t', 'fore the th', 'ore the the', 're the the ', 'e the the k', ' the the ki', 'the the kin', 'he the king', 'e the king ', ' the king w', 'the king wa', 'he king was', 'e king was ', ' king was i', 'king was in', 'ing was in ', 'ng was in h', 'g was in hi', ' was in his', 'was in his ', 'as in his c', 's in his co', ' in his cou', 'in his coun', 'n his count', ' his counti', 'his countin', 'is counting', 's counting ', ' counting c', 'counting co', 'ounting cou', 'unting coun', 'nting count', 'ting counti', 'ing countin', 'ng counting', 'g counting ', ' counting o', 'counting ou', 'ounting out', 'unting out ', 'nting out h', 'ting out hi', 'ing out his', 'ng out his ', 'g out his t', ' out his th', 'out his the', 'ut his the ', 't his the q', ' his the qu', 'his the que', 'is the quee', 's the queen', ' the queen ', 'the queen w', 'he queen wa', 'e queen was', ' queen was ', 'queen was i', 'ueen was in', 'een was in ', 'en was in t', 'n was in th', ' was in the', 'was in the ', 'as in the e', 's in the ea', ' in the eat', 'in the eati', 'n the eatin', ' the eating', 'the eating ', 'he eating b', 'e eating br', ' eating bre', 'eating brea', 'ating bread', 'ting bread ', 'ing bread a', 'ng bread an', 'g bread and', ' bread and ', 'bread and t', 'read and th', 'ead and the', 'ad and the ', 'd and the m', ' and the ma', 'and the mai', 'nd the maid', 'd the maid ', ' the maid w', 'the maid wa', 'he maid was', 'e maid was ', ' maid was i', 'maid was in', 'aid was in ', 'id was in t', 'd was in th', ' was in the', 'was in the ', 'as in the h', 's in the ha', ' in the han', 'in the hang', 'n the hangi', ' the hangin', 'the hanging', 'he hanging ', 'e hanging o', ' hanging ou', 'hanging out', 'anging out ', 'nging out t', 'ging out th', 'ing out the', 'ng out the ', 'g out the w', ' out the wh', 'out the whe', 'ut the when', 't the when ', ' the when d', 'the when do', 'he when dow', 'e when down', ' when down ', 'when down c', 'hen down ca', 'en down cam', 'n down came', ' down came ', 'down came a', 'own came a ', 'wn came a b', 'n came a bl', ' came a bla', 'came a blac', 'ame a black', 'me a blackb', 'e a blackbi', ' a blackbir', 'a blackbird', ' blackbird ', 'blackbird a', 'lackbird an', 'ackbird and', 'ckbird and ', 'kbird and p', 'bird and pe', 'ird and pec', 'rd and peck', 'd and pecke', ' and pecked', 'and pecked ', 'nd pecked o', 'd pecked of', ' pecked off', 'pecked off ', 'ecked off h', 'cked off he', 'ked off her']\n"
     ]
    }
   ],
   "source": [
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode Sequences\n",
    "The sequences of characters must be encoded as integers. This means that each unique character will be assigned a specific integer value and each sequence of characters will be encoded as a sequence of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can create the mapping given a sorted set of unique characters\n",
    "\n",
    "chars = sorted(list(set(raw_text)))\n",
    "mapping = dict((c,i) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'w': 22, 'y': 23}\n"
     ]
    }
   ],
   "source": [
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'w', 'y']\n"
     ]
    }
   ],
   "source": [
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding each character according to our above mapping\n",
    "encoded_sequences = list()\n",
    "\n",
    "for line in lines:\n",
    "    encode_seq = [mapping[char] for char in line]\n",
    "    \n",
    "    encoded_sequences.append(encode_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 10, 14, 8, 1, 2, 1, 19, 15, 14, 8]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(mapping)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into inputs and outputs \n",
    "import numpy as np\n",
    "encoded_sequences = np.array(encoded_sequences)\n",
    "X,y = encoded_sequences[:,:-1], encoded_sequences[:,-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19 10 14  8  1  2  1 19 15 14]\n",
      "8\n",
      "[10 14  8  1  2  1 19 15 14  8]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "print(y[0])\n",
    "print(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(289, 10)\n",
      "(289,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding each character . so each character becomes a vector as long as \n",
    "# the vocabulary (24 elements) with 1 marked for specific character\n",
    "# we use to_categorical() fn to one hot encode the input and output sequences\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "onehot_encoded_seq = [to_categorical(x,num_classes=vocab_size) for x in X]\n",
    "X = np.array(onehot_encoded_seq)\n",
    "\n",
    "y = to_categorical(y,num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(289, 10, 24)\n",
      "(289, 24)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "\n",
    "\n",
    "\n",
    "def define_model(X):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50,input_shape=(X.shape[1],X.shape[2])))\n",
    "    \n",
    "    model.add(Dense(50,activation='relu'))\n",
    "    model.add(Dense(100,activation='relu'))\n",
    "    model.add(Dense(150,activation='relu'))\n",
    "    model.add(Dense(vocab_size,activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 50)                15000     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 150)               15150     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 24)                3624      \n",
      "=================================================================\n",
      "Total params: 41,424\n",
      "Trainable params: 41,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "10/10 - 3s - loss: 3.1634 - accuracy: 0.1626\n",
      "Epoch 2/200\n",
      "10/10 - 0s - loss: 3.1022 - accuracy: 0.2111\n",
      "Epoch 3/200\n",
      "10/10 - 0s - loss: 2.9524 - accuracy: 0.2111\n",
      "Epoch 4/200\n",
      "10/10 - 0s - loss: 2.8090 - accuracy: 0.2111\n",
      "Epoch 5/200\n",
      "10/10 - 0s - loss: 2.7730 - accuracy: 0.2111\n",
      "Epoch 6/200\n",
      "10/10 - 0s - loss: 2.7280 - accuracy: 0.2111\n",
      "Epoch 7/200\n",
      "10/10 - 0s - loss: 2.7077 - accuracy: 0.2111\n",
      "Epoch 8/200\n",
      "10/10 - 0s - loss: 2.6776 - accuracy: 0.2111\n",
      "Epoch 9/200\n",
      "10/10 - 0s - loss: 2.6507 - accuracy: 0.2111\n",
      "Epoch 10/200\n",
      "10/10 - 0s - loss: 2.6395 - accuracy: 0.2111\n",
      "Epoch 11/200\n",
      "10/10 - 0s - loss: 2.6038 - accuracy: 0.2388\n",
      "Epoch 12/200\n",
      "10/10 - 0s - loss: 2.5731 - accuracy: 0.2457\n",
      "Epoch 13/200\n",
      "10/10 - 0s - loss: 2.5333 - accuracy: 0.2353\n",
      "Epoch 14/200\n",
      "10/10 - 0s - loss: 2.5010 - accuracy: 0.2353\n",
      "Epoch 15/200\n",
      "10/10 - 0s - loss: 2.4436 - accuracy: 0.2768\n",
      "Epoch 16/200\n",
      "10/10 - 0s - loss: 2.4055 - accuracy: 0.3010\n",
      "Epoch 17/200\n",
      "10/10 - 0s - loss: 2.3784 - accuracy: 0.2630\n",
      "Epoch 18/200\n",
      "10/10 - 0s - loss: 2.2827 - accuracy: 0.3114\n",
      "Epoch 19/200\n",
      "10/10 - 0s - loss: 2.2340 - accuracy: 0.3322\n",
      "Epoch 20/200\n",
      "10/10 - 0s - loss: 2.2692 - accuracy: 0.3529\n",
      "Epoch 21/200\n",
      "10/10 - 0s - loss: 2.1773 - accuracy: 0.3218\n",
      "Epoch 22/200\n",
      "10/10 - 0s - loss: 2.1317 - accuracy: 0.3495\n",
      "Epoch 23/200\n",
      "10/10 - 0s - loss: 2.0812 - accuracy: 0.3806\n",
      "Epoch 24/200\n",
      "10/10 - 0s - loss: 2.0328 - accuracy: 0.3737\n",
      "Epoch 25/200\n",
      "10/10 - 0s - loss: 1.9599 - accuracy: 0.4221\n",
      "Epoch 26/200\n",
      "10/10 - 0s - loss: 1.9185 - accuracy: 0.4256\n",
      "Epoch 27/200\n",
      "10/10 - 0s - loss: 1.9034 - accuracy: 0.4221\n",
      "Epoch 28/200\n",
      "10/10 - 0s - loss: 1.8420 - accuracy: 0.4429\n",
      "Epoch 29/200\n",
      "10/10 - 0s - loss: 1.7817 - accuracy: 0.4602\n",
      "Epoch 30/200\n",
      "10/10 - 0s - loss: 1.7774 - accuracy: 0.4291\n",
      "Epoch 31/200\n",
      "10/10 - 0s - loss: 1.8170 - accuracy: 0.4360\n",
      "Epoch 32/200\n",
      "10/10 - 0s - loss: 1.6901 - accuracy: 0.4740\n",
      "Epoch 33/200\n",
      "10/10 - 0s - loss: 1.7561 - accuracy: 0.4256\n",
      "Epoch 34/200\n",
      "10/10 - 0s - loss: 1.6045 - accuracy: 0.4913\n",
      "Epoch 35/200\n",
      "10/10 - 0s - loss: 1.5916 - accuracy: 0.5052\n",
      "Epoch 36/200\n",
      "10/10 - 0s - loss: 1.6727 - accuracy: 0.4810\n",
      "Epoch 37/200\n",
      "10/10 - 0s - loss: 1.5355 - accuracy: 0.5398\n",
      "Epoch 38/200\n",
      "10/10 - 0s - loss: 1.4903 - accuracy: 0.5294\n",
      "Epoch 39/200\n",
      "10/10 - 0s - loss: 1.5537 - accuracy: 0.5017\n",
      "Epoch 40/200\n",
      "10/10 - 0s - loss: 1.4628 - accuracy: 0.5398\n",
      "Epoch 41/200\n",
      "10/10 - 0s - loss: 1.4220 - accuracy: 0.5433\n",
      "Epoch 42/200\n",
      "10/10 - 0s - loss: 1.3643 - accuracy: 0.5675\n",
      "Epoch 43/200\n",
      "10/10 - 0s - loss: 1.3621 - accuracy: 0.5709\n",
      "Epoch 44/200\n",
      "10/10 - 0s - loss: 1.3442 - accuracy: 0.5813\n",
      "Epoch 45/200\n",
      "10/10 - 0s - loss: 1.3101 - accuracy: 0.5640\n",
      "Epoch 46/200\n",
      "10/10 - 0s - loss: 1.3632 - accuracy: 0.5398\n",
      "Epoch 47/200\n",
      "10/10 - 0s - loss: 1.4030 - accuracy: 0.5536\n",
      "Epoch 48/200\n",
      "10/10 - 0s - loss: 1.3081 - accuracy: 0.5606\n",
      "Epoch 49/200\n",
      "10/10 - 0s - loss: 1.2379 - accuracy: 0.6021\n",
      "Epoch 50/200\n",
      "10/10 - 0s - loss: 1.2262 - accuracy: 0.6159\n",
      "Epoch 51/200\n",
      "10/10 - 0s - loss: 1.2148 - accuracy: 0.5952\n",
      "Epoch 52/200\n",
      "10/10 - 0s - loss: 1.2388 - accuracy: 0.6055\n",
      "Epoch 53/200\n",
      "10/10 - 0s - loss: 1.2065 - accuracy: 0.6021\n",
      "Epoch 54/200\n",
      "10/10 - 0s - loss: 1.2212 - accuracy: 0.5952\n",
      "Epoch 55/200\n",
      "10/10 - 0s - loss: 1.1639 - accuracy: 0.6505\n",
      "Epoch 56/200\n",
      "10/10 - 0s - loss: 1.2508 - accuracy: 0.6125\n",
      "Epoch 57/200\n",
      "10/10 - 0s - loss: 1.2263 - accuracy: 0.6401\n",
      "Epoch 58/200\n",
      "10/10 - 0s - loss: 1.1312 - accuracy: 0.6471\n",
      "Epoch 59/200\n",
      "10/10 - 0s - loss: 1.1849 - accuracy: 0.6125\n",
      "Epoch 60/200\n",
      "10/10 - 0s - loss: 1.1314 - accuracy: 0.6436\n",
      "Epoch 61/200\n",
      "10/10 - 0s - loss: 1.0763 - accuracy: 0.6471\n",
      "Epoch 62/200\n",
      "10/10 - 0s - loss: 1.0913 - accuracy: 0.6644\n",
      "Epoch 63/200\n",
      "10/10 - 0s - loss: 1.0317 - accuracy: 0.6609\n",
      "Epoch 64/200\n",
      "10/10 - 0s - loss: 0.9751 - accuracy: 0.7024\n",
      "Epoch 65/200\n",
      "10/10 - 0s - loss: 0.9796 - accuracy: 0.6990\n",
      "Epoch 66/200\n",
      "10/10 - 0s - loss: 0.9608 - accuracy: 0.7232\n",
      "Epoch 67/200\n",
      "10/10 - 0s - loss: 0.9241 - accuracy: 0.6990\n",
      "Epoch 68/200\n",
      "10/10 - 0s - loss: 0.8799 - accuracy: 0.7093\n",
      "Epoch 69/200\n",
      "10/10 - 0s - loss: 1.0791 - accuracy: 0.6436\n",
      "Epoch 70/200\n",
      "10/10 - 0s - loss: 1.0004 - accuracy: 0.6817\n",
      "Epoch 71/200\n",
      "10/10 - 0s - loss: 0.9069 - accuracy: 0.7128\n",
      "Epoch 72/200\n",
      "10/10 - 0s - loss: 0.8204 - accuracy: 0.7405\n",
      "Epoch 73/200\n",
      "10/10 - 0s - loss: 0.7993 - accuracy: 0.7474\n",
      "Epoch 74/200\n",
      "10/10 - 0s - loss: 0.7375 - accuracy: 0.7716\n",
      "Epoch 75/200\n",
      "10/10 - 0s - loss: 0.8068 - accuracy: 0.7509\n",
      "Epoch 76/200\n",
      "10/10 - 0s - loss: 0.8316 - accuracy: 0.7163\n",
      "Epoch 77/200\n",
      "10/10 - 0s - loss: 0.8034 - accuracy: 0.7301\n",
      "Epoch 78/200\n",
      "10/10 - 0s - loss: 0.7196 - accuracy: 0.7820\n",
      "Epoch 79/200\n",
      "10/10 - 0s - loss: 0.6353 - accuracy: 0.8235\n",
      "Epoch 80/200\n",
      "10/10 - 0s - loss: 0.5954 - accuracy: 0.8235\n",
      "Epoch 81/200\n",
      "10/10 - 0s - loss: 0.5663 - accuracy: 0.8374\n",
      "Epoch 82/200\n",
      "10/10 - 0s - loss: 0.5861 - accuracy: 0.8304\n",
      "Epoch 83/200\n",
      "10/10 - 0s - loss: 0.7626 - accuracy: 0.7474\n",
      "Epoch 84/200\n",
      "10/10 - 0s - loss: 0.6499 - accuracy: 0.8062\n",
      "Epoch 85/200\n",
      "10/10 - 0s - loss: 0.6927 - accuracy: 0.7751\n",
      "Epoch 86/200\n",
      "10/10 - 0s - loss: 0.6182 - accuracy: 0.7958\n",
      "Epoch 87/200\n",
      "10/10 - 0s - loss: 0.8304 - accuracy: 0.7336\n",
      "Epoch 88/200\n",
      "10/10 - 0s - loss: 0.9498 - accuracy: 0.6990\n",
      "Epoch 89/200\n",
      "10/10 - 0s - loss: 0.8304 - accuracy: 0.7093\n",
      "Epoch 90/200\n",
      "10/10 - 0s - loss: 0.7426 - accuracy: 0.7336\n",
      "Epoch 91/200\n",
      "10/10 - 0s - loss: 0.6175 - accuracy: 0.7958\n",
      "Epoch 92/200\n",
      "10/10 - 0s - loss: 0.5189 - accuracy: 0.8408\n",
      "Epoch 93/200\n",
      "10/10 - 0s - loss: 0.4745 - accuracy: 0.8651\n",
      "Epoch 94/200\n",
      "10/10 - 0s - loss: 0.6634 - accuracy: 0.7820\n",
      "Epoch 95/200\n",
      "10/10 - 0s - loss: 0.6075 - accuracy: 0.7889\n",
      "Epoch 96/200\n",
      "10/10 - 0s - loss: 0.5595 - accuracy: 0.8166\n",
      "Epoch 97/200\n",
      "10/10 - 0s - loss: 0.7414 - accuracy: 0.7578\n",
      "Epoch 98/200\n",
      "10/10 - 0s - loss: 0.6905 - accuracy: 0.7716\n",
      "Epoch 99/200\n",
      "10/10 - 0s - loss: 0.6057 - accuracy: 0.7924\n",
      "Epoch 100/200\n",
      "10/10 - 0s - loss: 0.5646 - accuracy: 0.8339\n",
      "Epoch 101/200\n",
      "10/10 - 0s - loss: 0.5071 - accuracy: 0.8235\n",
      "Epoch 102/200\n",
      "10/10 - 0s - loss: 0.5100 - accuracy: 0.8685\n",
      "Epoch 103/200\n",
      "10/10 - 0s - loss: 0.4992 - accuracy: 0.8304\n",
      "Epoch 104/200\n",
      "10/10 - 0s - loss: 0.5015 - accuracy: 0.8062\n",
      "Epoch 105/200\n",
      "10/10 - 0s - loss: 0.5189 - accuracy: 0.8374\n",
      "Epoch 106/200\n",
      "10/10 - 0s - loss: 0.5406 - accuracy: 0.8235\n",
      "Epoch 107/200\n",
      "10/10 - 0s - loss: 0.4333 - accuracy: 0.8685\n",
      "Epoch 108/200\n",
      "10/10 - 0s - loss: 0.3561 - accuracy: 0.9066\n",
      "Epoch 109/200\n",
      "10/10 - 0s - loss: 0.3131 - accuracy: 0.9135\n",
      "Epoch 110/200\n",
      "10/10 - 0s - loss: 0.3067 - accuracy: 0.9204\n",
      "Epoch 111/200\n",
      "10/10 - 0s - loss: 0.3207 - accuracy: 0.9031\n",
      "Epoch 112/200\n",
      "10/10 - 0s - loss: 0.4747 - accuracy: 0.8408\n",
      "Epoch 113/200\n",
      "10/10 - 0s - loss: 0.3539 - accuracy: 0.9066\n",
      "Epoch 114/200\n",
      "10/10 - 0s - loss: 0.3446 - accuracy: 0.8893\n",
      "Epoch 115/200\n",
      "10/10 - 0s - loss: 0.2746 - accuracy: 0.9273\n",
      "Epoch 116/200\n",
      "10/10 - 0s - loss: 0.2479 - accuracy: 0.9412\n",
      "Epoch 117/200\n",
      "10/10 - 0s - loss: 0.2203 - accuracy: 0.9481\n",
      "Epoch 118/200\n",
      "10/10 - 0s - loss: 0.1998 - accuracy: 0.9585\n",
      "Epoch 119/200\n",
      "10/10 - 0s - loss: 0.1929 - accuracy: 0.9550\n",
      "Epoch 120/200\n",
      "10/10 - 0s - loss: 0.1815 - accuracy: 0.9689\n",
      "Epoch 121/200\n",
      "10/10 - 0s - loss: 0.2105 - accuracy: 0.9654\n",
      "Epoch 122/200\n",
      "10/10 - 0s - loss: 0.2047 - accuracy: 0.9516\n",
      "Epoch 123/200\n",
      "10/10 - 0s - loss: 0.1816 - accuracy: 0.9723\n",
      "Epoch 124/200\n",
      "10/10 - 0s - loss: 0.2421 - accuracy: 0.9308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/200\n",
      "10/10 - 0s - loss: 0.2655 - accuracy: 0.9031\n",
      "Epoch 126/200\n",
      "10/10 - 0s - loss: 0.2520 - accuracy: 0.9446\n",
      "Epoch 127/200\n",
      "10/10 - 0s - loss: 0.2128 - accuracy: 0.9412\n",
      "Epoch 128/200\n",
      "10/10 - 0s - loss: 0.1864 - accuracy: 0.9585\n",
      "Epoch 129/200\n",
      "10/10 - 0s - loss: 0.1523 - accuracy: 0.9723\n",
      "Epoch 130/200\n",
      "10/10 - 0s - loss: 0.1364 - accuracy: 0.9792\n",
      "Epoch 131/200\n",
      "10/10 - 0s - loss: 0.1231 - accuracy: 0.9827\n",
      "Epoch 132/200\n",
      "10/10 - 0s - loss: 0.1085 - accuracy: 0.9862\n",
      "Epoch 133/200\n",
      "10/10 - 0s - loss: 0.1037 - accuracy: 0.9827\n",
      "Epoch 134/200\n",
      "10/10 - 0s - loss: 0.1281 - accuracy: 0.9792\n",
      "Epoch 135/200\n",
      "10/10 - 0s - loss: 0.1152 - accuracy: 0.9862\n",
      "Epoch 136/200\n",
      "10/10 - 0s - loss: 0.1004 - accuracy: 0.9896\n",
      "Epoch 137/200\n",
      "10/10 - 0s - loss: 0.1602 - accuracy: 0.9550\n",
      "Epoch 138/200\n",
      "10/10 - 0s - loss: 0.3865 - accuracy: 0.8512\n",
      "Epoch 139/200\n",
      "10/10 - 0s - loss: 0.2887 - accuracy: 0.9100\n",
      "Epoch 140/200\n",
      "10/10 - 0s - loss: 0.1896 - accuracy: 0.9516\n",
      "Epoch 141/200\n",
      "10/10 - 0s - loss: 0.1487 - accuracy: 0.9689\n",
      "Epoch 142/200\n",
      "10/10 - 0s - loss: 0.1315 - accuracy: 0.9758\n",
      "Epoch 143/200\n",
      "10/10 - 0s - loss: 0.2464 - accuracy: 0.9412\n",
      "Epoch 144/200\n",
      "10/10 - 0s - loss: 0.2120 - accuracy: 0.9516\n",
      "Epoch 145/200\n",
      "10/10 - 0s - loss: 0.1376 - accuracy: 0.9758\n",
      "Epoch 146/200\n",
      "10/10 - 0s - loss: 0.2894 - accuracy: 0.9170\n",
      "Epoch 147/200\n",
      "10/10 - 0s - loss: 0.1660 - accuracy: 0.9550\n",
      "Epoch 148/200\n",
      "10/10 - 0s - loss: 0.1120 - accuracy: 0.9896\n",
      "Epoch 149/200\n",
      "10/10 - 0s - loss: 0.0941 - accuracy: 0.9896\n",
      "Epoch 150/200\n",
      "10/10 - 0s - loss: 0.0698 - accuracy: 0.9931\n",
      "Epoch 151/200\n",
      "10/10 - 0s - loss: 0.0691 - accuracy: 0.9862\n",
      "Epoch 152/200\n",
      "10/10 - 0s - loss: 0.0591 - accuracy: 0.9896\n",
      "Epoch 153/200\n",
      "10/10 - 0s - loss: 0.0644 - accuracy: 0.9827\n",
      "Epoch 154/200\n",
      "10/10 - 0s - loss: 0.0642 - accuracy: 0.9862\n",
      "Epoch 155/200\n",
      "10/10 - 0s - loss: 0.0844 - accuracy: 0.9862\n",
      "Epoch 156/200\n",
      "10/10 - 0s - loss: 0.0804 - accuracy: 0.9896\n",
      "Epoch 157/200\n",
      "10/10 - 0s - loss: 0.0896 - accuracy: 0.9827\n",
      "Epoch 158/200\n",
      "10/10 - 0s - loss: 0.0718 - accuracy: 0.9792\n",
      "Epoch 159/200\n",
      "10/10 - 0s - loss: 0.0574 - accuracy: 0.9827\n",
      "Epoch 160/200\n",
      "10/10 - 0s - loss: 0.0475 - accuracy: 0.9896\n",
      "Epoch 161/200\n",
      "10/10 - 0s - loss: 0.0402 - accuracy: 0.9862\n",
      "Epoch 162/200\n",
      "10/10 - 0s - loss: 0.0419 - accuracy: 0.9931\n",
      "Epoch 163/200\n",
      "10/10 - 0s - loss: 0.0395 - accuracy: 0.9896\n",
      "Epoch 164/200\n",
      "10/10 - 0s - loss: 0.0400 - accuracy: 0.9931\n",
      "Epoch 165/200\n",
      "10/10 - 0s - loss: 0.0573 - accuracy: 0.9792\n",
      "Epoch 166/200\n",
      "10/10 - 0s - loss: 0.0759 - accuracy: 0.9896\n",
      "Epoch 167/200\n",
      "10/10 - 0s - loss: 0.0769 - accuracy: 0.9827\n",
      "Epoch 168/200\n",
      "10/10 - 0s - loss: 0.0504 - accuracy: 0.9931\n",
      "Epoch 169/200\n",
      "10/10 - 0s - loss: 0.0412 - accuracy: 0.9896\n",
      "Epoch 170/200\n",
      "10/10 - 0s - loss: 0.0366 - accuracy: 0.9896\n",
      "Epoch 171/200\n",
      "10/10 - 0s - loss: 0.0332 - accuracy: 0.9931\n",
      "Epoch 172/200\n",
      "10/10 - 0s - loss: 0.0325 - accuracy: 0.9931\n",
      "Epoch 173/200\n",
      "10/10 - 0s - loss: 0.0337 - accuracy: 0.9896\n",
      "Epoch 174/200\n",
      "10/10 - 0s - loss: 0.0370 - accuracy: 0.9896\n",
      "Epoch 175/200\n",
      "10/10 - 0s - loss: 0.0381 - accuracy: 0.9862\n",
      "Epoch 176/200\n",
      "10/10 - 0s - loss: 0.0340 - accuracy: 0.9862\n",
      "Epoch 177/200\n",
      "10/10 - 0s - loss: 0.0258 - accuracy: 0.9931\n",
      "Epoch 178/200\n",
      "10/10 - 0s - loss: 0.0351 - accuracy: 0.9896\n",
      "Epoch 179/200\n",
      "10/10 - 0s - loss: 0.0260 - accuracy: 0.9931\n",
      "Epoch 180/200\n",
      "10/10 - 0s - loss: 0.0296 - accuracy: 0.9896\n",
      "Epoch 181/200\n",
      "10/10 - 0s - loss: 0.0286 - accuracy: 0.9896\n",
      "Epoch 182/200\n",
      "10/10 - 0s - loss: 0.0269 - accuracy: 0.9931\n",
      "Epoch 183/200\n",
      "10/10 - 0s - loss: 0.0307 - accuracy: 0.9896\n",
      "Epoch 184/200\n",
      "10/10 - 0s - loss: 0.0339 - accuracy: 0.9896\n",
      "Epoch 185/200\n",
      "10/10 - 0s - loss: 0.0342 - accuracy: 0.9931\n",
      "Epoch 186/200\n",
      "10/10 - 0s - loss: 0.0282 - accuracy: 0.9931\n",
      "Epoch 187/200\n",
      "10/10 - 0s - loss: 0.0256 - accuracy: 0.9931\n",
      "Epoch 188/200\n",
      "10/10 - 0s - loss: 0.0317 - accuracy: 0.9896\n",
      "Epoch 189/200\n",
      "10/10 - 0s - loss: 0.0254 - accuracy: 0.9931\n",
      "Epoch 190/200\n",
      "10/10 - 0s - loss: 0.0358 - accuracy: 0.9931\n",
      "Epoch 191/200\n",
      "10/10 - 0s - loss: 0.0274 - accuracy: 0.9965\n",
      "Epoch 192/200\n",
      "10/10 - 0s - loss: 0.0292 - accuracy: 0.9896\n",
      "Epoch 193/200\n",
      "10/10 - 0s - loss: 0.0240 - accuracy: 0.9931\n",
      "Epoch 194/200\n",
      "10/10 - 0s - loss: 0.0291 - accuracy: 0.9931\n",
      "Epoch 195/200\n",
      "10/10 - 0s - loss: 0.0226 - accuracy: 0.9931\n",
      "Epoch 196/200\n",
      "10/10 - 0s - loss: 0.0279 - accuracy: 0.9931\n",
      "Epoch 197/200\n",
      "10/10 - 0s - loss: 0.0252 - accuracy: 0.9931\n",
      "Epoch 198/200\n",
      "10/10 - 0s - loss: 0.0290 - accuracy: 0.9931\n",
      "Epoch 199/200\n",
      "10/10 - 0s - loss: 0.0292 - accuracy: 0.9896\n",
      "Epoch 200/200\n",
      "10/10 - 0s - loss: 0.0302 - accuracy: 0.9896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ec7e5d85e0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = define_model(X)\n",
    "model.fit(X,y,epochs=200,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('CharBasedLanguageModeling.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the mapping \n",
    "from pickle import dump\n",
    "dump(mapping,open('mapping.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must provide sequences of 10 characters as input to the model in order to start the generation process. We will pick these manually. A given input sequence will need to be prepared in the same way as preparing the training data for the model. \n",
    "We will use the following steps in order to do it\n",
    "\n",
    "1) the sequence of characters must be integer encoded using the loaded mapping\n",
    "\n",
    "2) the integers need to be one hot encoded using the to categorical() Keras function\n",
    "\n",
    "3) reshape the sequence to be 3-dimensional, as we only have one sequence and LSTMs require all input to be three dimensional (samples, time steps, features).\n",
    "\n",
    "4)model to predict the next character in the sequence. We use\n",
    "predict classes() instead of predict() to directly select the integer for the character with the highest probability instead of getting the full probability distribution across the entire set of characters\n",
    "\n",
    "5) We can then decode this integer by looking up the mapping to see the character to which it maps.\n",
    "\n",
    "6) This character can then be added to the input sequence. We then need to make sure that the input sequence is 10 characters by truncating the first character from the input sequence text.\n",
    "We can use the pad sequences() function from the Keras API that can perform this truncation\n",
    "operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence of characters with a language model \n",
    "\n",
    "def generate_seq(model,mapping, seq_length, seed_text, n_chars):\n",
    "    in_txt = clean_data(seed_text)\n",
    "    \n",
    "    #genarating n number of chars\n",
    "    for _ in range(n_chars):\n",
    "        \n",
    "        #encode the text as integers\n",
    "        encoded_seq = [mapping[char] for char in in_txt]\n",
    "        \n",
    "        #truncate sequences to a fixed legth \n",
    "        encoded_seq = pad_sequences([encoded_seq],maxlen=seq_length,truncating='pre')\n",
    "        \n",
    "        #one hot encoding\n",
    "        encoded_seq = to_categorical(encoded_seq,num_classes=len(mapping))\n",
    "        \n",
    "        #reshaping \n",
    "        #encoded_seq = encoded_seq.reshape(1,encoded_seq.shape[0],encoded_seq.shape[1])\n",
    "        \n",
    "        yhat = model.predict_classes(encoded_seq)\n",
    "       # print(yhat)\n",
    "        \n",
    "        #Integer to character\n",
    "        out_char = ''\n",
    "        for char,i in mapping.items():\n",
    "            if i == yhat:\n",
    "                out_char = char\n",
    "                break\n",
    "        in_txt += out_char\n",
    "        \n",
    "    return in_txt\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('CharBasedLanguageModeling.h5')\n",
    "mapping = load(open('mapping.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sing a song of a pocket full o\n",
      "king was in his counting count\n",
      "hello world of crfackddnd pblkb\n"
     ]
    }
   ],
   "source": [
    "# test start of rhyme\n",
    "print(generate_seq(model, mapping, 10, 'Sing a son', 20))\n",
    "# test mid-line\n",
    "print(generate_seq(model, mapping, 10, 'king was i', 20))\n",
    "# test not in original\n",
    "print(generate_seq(model, mapping, 10, 'hello world', 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sing a son\n"
     ]
    }
   ],
   "source": [
    "in_txt = clean_data('Sing a son')\n",
    "print(in_txt)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 10, 14, 8, 1, 2, 1, 19, 15, 14]\n"
     ]
    }
   ],
   "source": [
    "encoded_seq = [mapping[char] for char in in_txt]\n",
    "print(encoded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19 10 14  8  1  2  1 19 15 14]]\n"
     ]
    }
   ],
   "source": [
    "encoded_seq = pad_sequences([encoded_seq],maxlen=10,truncating='pre')\n",
    "print(encoded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 24)\n"
     ]
    }
   ],
   "source": [
    "encoded_seq = to_categorical(encoded_seq,num_classes=len(mapping))\n",
    "print(encoded_seq.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('\\n', 0), (' ', 1), ('a', 2), ('b', 3), ('c', 4), ('d', 5), ('e', 6), ('f', 7), ('g', 8), ('h', 9), ('i', 10), ('k', 11), ('l', 12), ('m', 13), ('n', 14), ('o', 15), ('p', 16), ('q', 17), ('r', 18), ('s', 19), ('t', 20), ('u', 21), ('w', 22), ('y', 23)])\n"
     ]
    }
   ],
   "source": [
    "print(mapping.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
